{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is a Neural Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.ibb.co/d79YnGD/Screenshot-from-2019-04-17-15-58-08.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As shown in the above image, basically a neural network is a neuron (evaluated using Logistic Regression) repeated multiple times\n",
    "- In neural network notation, we don't count the input layer. So, the above shown neural network is a **\"2 layer NN\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Neural Network Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.ibb.co/T0VYmkT/Screenshot-from-2019-04-17-16-06-35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Computing a Neural Network's Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The hidden layer has four neurons, so the output or activation from the hidden layer is a column vector with four single valued rows and is denoted by **a[1]** as it is the first layer of the network\n",
    "- In logistic regression, when input is X, a neuron computes the following two equations to get **z & a**:\n",
    "    - z = wx + b\n",
    "    - a = sigmoid(x)\n",
    "    - yhat = a (output of the neuron/layer)\n",
    "    - **Size and dimension of the layers depends on the number of neurons contained by the layer**\n",
    "\n",
    "![](https://i.ibb.co/0BQqjXc/Screenshot-from-2019-04-17-16-50-35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, for a neural network with hidden layers and multiple neurons, this is how the weights and biases are calculated\n",
    "- First the input layer (input values = X), is multiplied with the transpose of the first layer weights W[1].T, and passed through a sigmoid function to get the output from the first hidden layer (this implements Logistic Regression on the first layer)\n",
    "    - Same thing is repeated for the second layer where the output from the first acts as input to the second\n",
    "- From the output of the output layer, the cost function L is evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.ibb.co/q96kxQX/Screenshot-from-2019-04-17-16-53-42.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Final algorithm to implement the neural network for one example at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.ibb.co/ygS5JcW/Screenshot-from-2019-04-17-17-00-41.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Vectorizing across multiple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.ibb.co/BCykzTw/Screenshot-from-2019-04-17-17-07-46.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As shown in the above image, to train all the samples from a training set, we just stack each data point horizontally to create a matrix X\n",
    "    - X = [X1 X2 ... Xm]\n",
    "- Similarly, the Z vector is evaluated using the formula **z = w.T + b** for each column of the vector X to finally form matrix Z\n",
    "    - Z = [Z1 Z2 ... Zm]\n",
    "    - For each layer, there is a separate Z matrix\n",
    "    - e.g. Z[1], Z[2] represent the first and second layers of the network\n",
    "- The final output matrix of a layer A is the sigmoid of the matrix Z\n",
    "    - A = sigmoid(Z)\n",
    "    - This is the output of a layer and acts an input to the second layer\n",
    "    - If we go down vertically in a column of matrix A, it represents the activations from nodes of that hidden/output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation about the dimensions of the vectors W, X, Z, and A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vector X is formed by stacking all the data points horizontally.\n",
    "    - X = [x1 x2 ... xm], where \"m\" is the number of samples\n",
    "    - Dimension of X is **(nx, m)**\n",
    "        - nx: the number of features in a data point\n",
    "        - m: the number of training samples\n",
    "\n",
    "- Vector W is formed by stacking the number of neurons/nodes in the layer for each data point of X\n",
    "    - W = [w1 w2 ... wm], where the number of rows is the number of nodes in the layer\n",
    "    - So, W.T is the transpose of W to make it compatible for multiplication with X\n",
    "    - Dimension of W.T is **(k, nx)**\n",
    "        - k: the number of nodes in the layer\n",
    "        - nx: the number of features in a data point\n",
    "        \n",
    "- Vector Z = W.T * X\n",
    "    - Its dimension is (k, nx) * (nx, m) = **(k, m)**\n",
    "    - k: the number of nodes in the layer\n",
    "    - m: the number of training samples\n",
    "\n",
    "- Vector A = sigmoid(Z)\n",
    "    - A is the result of using an activation function over Z to make the output in a range 0-1 (which is what the sigmoid does)\n",
    "    - Dimension is the same as that of Z, i.e. **(k, m)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hyperbolic tangent function almost always works better than a sigmoid function\n",
    "    - Sigmoid has an output range (0, 1) and tanh function has an output range (-1, 1)\n",
    "    - Only place where a sigmoid function can be useful is at the output layer of a binary classification, where you want the output to be between (0, 1)\n",
    "    \n",
    "- One of the downsides of both the sigmoid and tanh functions is that if the value of **z** is very large or very small, the slope of the function approximates to nearly zero. This can drastically slow down the gradient descent and can hinder convergence in those cases.\n",
    "\n",
    "\n",
    "- **RULE OF THUMB**\n",
    "    - Just use **Relu (Rectified Linear Unit)** function for all hidden layers and only use sigmoid at the output layer if you are trying to implement a binary classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.ibb.co/XLNFmTX/Screenshot-from-2019-04-18-17-06-27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sometimes, leaky relu performs better than relu, but relu is the ultimate choice in most cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asheesh",
   "language": "python",
   "name": "asheesh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
