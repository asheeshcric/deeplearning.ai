{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient descent is an algorithm in machine learning that is used to evaluate the parameters that are used in the model\n",
    "- The main downside of Gradient Descent is that it has to go through the entire training set on each descent (or iteration)\n",
    "    - So, if the training dataset used is very large, then the algorithm takes huge amount of time\n",
    "- To mitigate this problem, we use **mini-batch gradient descent**, taking batches from training data for each descent\n",
    "    - This helps gradient descent to progress smoothly by not requiring the entire training dataset on each step (or descent)\n",
    "- A mini-batch from the training set is represented as $X^{\\{t\\}}$, $Y^{\\{t\\}}$ (we use curly braces to represent the $t^{th}$ mini-batch)\n",
    "![](https://i.ibb.co/ZVnG6dD/Screenshot-from-2019-07-15-11-52-11.png)\n",
    "\n",
    "- Each step of the descent is on a mini-batch instead of the whole training set\n",
    "- Size of each mini-batch is the size of the dataset in each loop\n",
    "- 1 epoch = A single pass through the entire training set (going through all mini-batches of the set for once)\n",
    "    - The only difference is that the gradient descent gets updated after each mini-batch is processed within a running epoch unlike the full gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of Mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Size = \"m\": Batch Gradient Descent (too slow although has better accuracy)\n",
    "- Size = \"1\": Stochastic Gradient Descent\n",
    "    - Too prone to noise and outliers\n",
    "    - We lose the advantage of speed of vectorization as each mini-batch is only a single example\n",
    "- Size = \"between 1 and m\": Generally taken size in practice are 64, 128, 256, 512, 1024\n",
    "\n",
    "- Hence, size of mini-batch is also another **hyperparameter** to consider"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asheesh",
   "language": "python",
   "name": "asheesh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
