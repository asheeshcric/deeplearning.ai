{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient descent is an algorithm in machine learning that is used to evaluate the parameters that are used in the model\n",
    "- The main downside of Gradient Descent is that it has to go through the entire training set on each descent (or iteration)\n",
    "    - So, if the training dataset used is very large, then the algorithm takes huge amount of time\n",
    "- To mitigate this problem, we use **mini-batch gradient descent**, taking batches from training data for each descent\n",
    "    - This helps gradient descent to progress smoothly by not requiring the entire training dataset on each step (or descent)\n",
    "- A mini-batch from the training set is represented as $X^{\\{t\\}}$, $Y^{\\{t\\}}$ (we use curly braces to represent the $t^{th}$ mini-batch)\n",
    "![](https://i.ibb.co/ZVnG6dD/Screenshot-from-2019-07-15-11-52-11.png)\n",
    "\n",
    "- Each step of the descent is on a mini-batch instead of the whole training set\n",
    "- Size of each mini-batch is the size of the dataset in each loop\n",
    "- 1 epoch = A single pass through the entire training set (going through all mini-batches of the set for once)\n",
    "    - The only difference is that the gradient descent gets updated after each mini-batch is processed within a running epoch unlike the full gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of Mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Size = \"m\": Batch Gradient Descent (too slow although has better accuracy)\n",
    "- Size = \"1\": Stochastic Gradient Descent\n",
    "    - Too prone to noise and outliers\n",
    "    - We lose the advantage of speed of vectorization as each mini-batch is only a single example\n",
    "- Size = \"between 1 and m\": Generally taken sizes in practice are among 64, 128, 256, 512, 1024, etc\n",
    "\n",
    "- Hence, size of mini-batch is also another **hyperparameter** to consider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exponentially Weighted Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Used on basically any data that is in sequence\n",
    "- It is also referred as **smoothing** of the data (or timeseries)\n",
    "    - $v_{t}$ = $\\beta$$v_{t-1}$ + (1 - $\\beta$)$\\theta_{t}$\n",
    "- Generally, we take $\\beta$ = 0.9 for practical consideration\n",
    "![](https://i.ibb.co/7VmTvRh/Screenshot-from-2019-07-16-10-15-20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In exponentially weighted averages, when the initial value $v_{0}$ = 0, then it can create an unwanted bias making the initial averahes to be much lower than the actual. So, we use the following formula in place of $v_{t}$\n",
    "    - $v_{t}$ = $v_{t}$ / (1 - $\\beta^{t}$)\n",
    "    - This is required for bias correction and not letting the initial values be affected by a fixed bias towards **zero** or **origin**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gradient Descent with Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While using **mini-batch gradient descent**, the parameters get updated after each mini-batch cycle (having some variance\n",
    "in each update). This make the gradient descent to oscillate a lot while moving towards the convergence\n",
    "- So, gradient descent with momentum computes an exponentially weighted averages of the gradients and then use that gradient\n",
    "to update the weights instead\n",
    "    - This helps in reducing the oscillations during G.D. and makes the convergence faster\n",
    "    > $v_{dw}$ = $\\beta$ $v_{dw}$ + (1 - $\\beta$)dw\n",
    "    \n",
    "    > $v_{db}$ = $\\beta$ $v_{db}$ + (1 - $\\beta$)db\n",
    "    \n",
    "    > w = w - $\\alpha$$v_{dw}$  &  b = b - $\\alpha$$v_{db}$\n",
    "    \n",
    "- So, here \"$\\beta$\" is a new hyperparameter involved which basically carries out exponentially weighted averages on each update making the convergence faster\n",
    "    - Generally, the practically considered value of $\\beta$ is ~ 0.9\n",
    "- Hence, this method is basically taking *\"exponentially weighted moving averages\"* method and merging\n",
    "it to the *\"mini-batch gradient descent\"* algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. RMSprop Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RMSprop is quite similar to G.D with Momentum except for the fact that it restricts the oscillations in the vertical direction\n",
    "    - This allows the descent to take greater leaps in the horizontal direction with greater **learning rate** as the vertical movement is restricted\n",
    "    \n",
    "- In this case, the exponentially weighted moving averages are calculated differently as shown below\n",
    "    > $s_{dw}$ = $\\beta$ $s_{dw}$ + (1 - $\\beta$) $dw^{2}$  \n",
    "    > $s_{db}$ = $\\beta$ $s_{db}$ + (1 - $\\beta$) $db^{2}$   \n",
    "    > w = w - $\\alpha$ * dw / $\\sqrt{s_{dw} + \\epsilon}$  \n",
    "    > b = b - $\\alpha$ * db / $\\sqrt{s_{db} + \\epsilon}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RMSprop and Momentum algorithms both decrease the vertical oscillations and increase horizontal speed, making the descent converge faster for a given cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Adam Optimization Algorithm (RMSprop + Momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It combines the techniques from both RMSprop and Momentum algorithms to calculate the gradients\n",
    "- The term **Adam** is derived from **Adaptive Moment Estimation**\n",
    "- First, it calculates gradients using the momentum method:\n",
    "    > $v_{dw}$ = $\\beta_{1}$ $v_{dw}$ + (1 - $\\beta_{1}$)dw  \n",
    "    > $v_{db}$ = $\\beta_{1}$ $v_{db}$ + (1 - $\\beta_{1}$)db\n",
    "    >\n",
    "    > $v_{dw_{c}}$ = $v_{dw}$ / (1 - $\\beta_{1}^{t}$),  <-- *where  $v_{dw_{c}}$ is the corrected form of $v_{dw}$*\n",
    "    >\n",
    "    > $v_{db_{c}}$ = $v_{db}$ / (1 - $\\beta_{1}^{t}$),  <-- *where  $v_{db_{c}}$ is the corrected form of $v_{db}$*\n",
    "    \n",
    "- Then we have the gradients using the RMSprop method:\n",
    "    > $s_{dw}$ = $\\beta_{2}$ $s_{dw}$ + (1 - $\\beta_{2}$) $dw^{2}$  \n",
    "    > $s_{db}$ = $\\beta_{2}$ $s_{db}$ + (1 - $\\beta_{2}$) $db^{2}$\n",
    "    >\n",
    "    > $s_{dw_{c}}$ = $s_{dw}$ / (1 - $\\beta_{2}^{t}$),  <-- *where  $s_{dw_{c}}$ is the corrected form of $s_{dw}$*\n",
    "    >\n",
    "    > $s_{db_{c}}$ = $s_{db}$ / (1 - $\\beta_{2}^{t}$),  <-- *where  $s_{db_{c}}$ is the corrected form of $s_{db}$*\n",
    "    \n",
    "    \n",
    "- Finally the weights are updated as follows:\n",
    "    > w = w - $\\alpha$ *  $v_{dw_{c}}$ / $\\sqrt{s_{dw_{c}} + epsilon}$\n",
    "    >\n",
    "    > b = b - $\\alpha$ *  $v_{db_{c}}$ / $\\sqrt{s_{db_{c}} + epsilon}$\n",
    "    \n",
    "- Here, the hyperparameters are $\\alpha$, $\\beta_{1}$ = 0.9, $\\beta_{2}$ = 0.999, and $\\epsilon$ = $10^{-8}$ with practical use-case values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Learning Rate Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- During gradient descent, the pathway may oscillate around the minimum if the learning rate is sufficiently large to avoid convergence.\n",
    "- So, there is a technique to lower down the learning rate as it approaches the minimum so that it converges faster\n",
    "- The formula for a **decaying learning rate** is given below:\n",
    "\n",
    "> $\\alpha$ = $\\frac{\\alpha_{o}}{1\\; + \\;decay\\_rate\\; *  \\;epoch\\_num}$\n",
    "\n",
    "- Here, $\\alpha_{0}$ is the initial learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other learning rate formulae that can be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\alpha$ = $\\alpha_{o}$ * $0.95^{\\:epoch\\_num}$\n",
    "\n",
    "- Discrete Staircase\n",
    "\n",
    "> $\\alpha$ = $\\frac{k\\: *\\: \\alpha_{o}}{\\sqrt{epoch\\_num}}\\;\\;$ or $\\;\\frac{k * \\:\\alpha_{o}}{\\sqrt{t}}$\n",
    "\n",
    "- One option is to manually decay the learning rate during the training process which is not feasible most of the times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generally there are **local optimum** during gradient descent which are also called **saddle points** where the pathway of the descent may get stuck not resulting in a convergence\n",
    "    - So, we should be aware of such points in the descent\n",
    "    \n",
    "- Also, plateaus in the learning curve may make the learning slow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asheesh",
   "language": "python",
   "name": "asheesh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
