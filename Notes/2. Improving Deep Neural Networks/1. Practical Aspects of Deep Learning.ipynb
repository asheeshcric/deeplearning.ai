{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train/dev/test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generally, when training a neural network, we divide the entire dataset into three parts:\n",
    "    - Training set\n",
    "    - Dev (or Validation) set\n",
    "    - Testing set\n",
    "    \n",
    "- When the size of the dataset is large (~ 1 million or above), the ratio of the sets is usually taken as 98:1:1\n",
    "\n",
    "- Dev or validation set is used to tune the network and hence it can get overfitted with the network that we are training on\n",
    "\n",
    "- Dev set and Test set should come from the same distribution for proper validation of the neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bias and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bias in machine learning tends to denote how well our trained network did well on the training set\n",
    "    - High bias refers to higher training error rate in a neural network\n",
    "    - Generally, high bias is a result of **underfitting**\n",
    "        - This means that the model is too weak to understand the complicated relation of your training set and the target set\n",
    "        \n",
    "- Variance on the other hand refers to how well our trained model is able to generalize on new data (i.e. data it has never seen before)\n",
    "    - If a model has overfit its training set, it does not do well with the test set and new data that it encounters\n",
    "    - High variance is a sign for **overfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Basic Recipe of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. High Bias**\n",
    "\n",
    "- When there is a high training error rate on the model we trained, then it is said to have **high bias**\n",
    "    - It reflects the inability of the model to learn precisely from its data\n",
    "- High bias generally occurs due to the use of a simpler model for a dataset with complicated relationship\n",
    "- Tips for preventing High Bias:\n",
    "    - Making the network larger\n",
    "        - Generally, when a smaller (or shallow) neural network is used (which covers only linear relationships), it is not able to account for the complicated relationship that our dataset has\n",
    "        - This leads to **underfitting**, where all the features of the dataset are not learned by the model\n",
    "        - So, increasing the size of the model used or going deeper into the network can make the model perform better on the given dataset\n",
    "        \n",
    "    - Manipulating the Neural Network structure\n",
    "        - Sometimes, interchanging the layers used in the network with one another can also do the trick to learn more essential features of the dataset\n",
    "        \n",
    "        \n",
    "**b. High Variance**\n",
    "\n",
    "- Variance occurs when the model performs well on the training set but does not generalize well for the test set or new data\n",
    "- Greater the difference between the training error rate and the test error rate, greater is the variance for the model\n",
    "- High variance for a model indicates that the model has simply crammed the entire training set and is unlikely to perform well on the data that it has not seen before\n",
    "- This condition is called **overfitting** where the model performs well only on the training set and not on the testing set\n",
    "\n",
    "- Tips for preventing High Variance\n",
    "    - Include more training data\n",
    "        - Simply introducing more data in the training set can remove overfitting of a complicated model as it previously may not have sufficient data to generalize well\n",
    "        - New data can mean that the model needs to learn newer features and can somehow not overfit the entire training set\n",
    "        \n",
    "    - Smaller Network\n",
    "        - Sometimes a complicated network is large enough to remember all the feature relationships the dataset we are training on\n",
    "        - This generally results in overfitting as it learns the entire dataset with ease\n",
    "        - So reducing the number of layers in the model or altering the layer size can help the model to not overfit on the training dataset\n",
    "        \n",
    "    - Regularization\n",
    "        - The best technique to avoid overfitting the data and prevent high variance is to use a **regularization** technique\n",
    "        - Regularization techniques tend to decrease the quantitative increase in the weight matrix of a neural network. This helps in not allowing the neural network to learn all the data in the training set\n",
    "        - Some of the regularization techniques are L1/L2 Regularization, Dropout Regularization, Data Augmentation, Early Stopping, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When overfitting occurs during training a model, it is best to use a suitable **regularization** technique that can prevent it\n",
    "- There are generally two types of **regularization** techniques used widely (except in CNNs) are:\n",
    "    - L1 Regularization -- Lasso Regression\n",
    "    - L2 Regularization -- Ridge Regression\n",
    "    \n",
    "- **Lasso (Least Absolute Shrinkage and Selection Operator) Regression / L1 Regularization**\n",
    "    - This technique adds **absolute value of magnitude** (mag(W)) of the weight matrix as penalty term to the loss function (J)\n",
    "    - It shrinks the less important feature's coefficient to zero thus, removing some features altogether\n",
    "        - Hence, it is generally used for **feature selection** in case we have a huge number of features\n",
    "- **Ridge Regression / L2 Regularization**\n",
    "    - It adds **squared magnitude** (W^2) as penalty term to the loss function\n",
    "    - If the value of **lambda** is too large, it will shrink the Weight matrix of the neural network to nearly zero which can make the model underfit the dataset\n",
    "    - This technique works very well to avoid the overfitting issue\n",
    "    - For neural networks, the L2 norm is often called **Frobenius Norm**\n",
    "    \n",
    "\n",
    "![](https://i.ibb.co/jgC2rT1/Screenshot-from-2019-05-15-20-08-36.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Why regularization prevents overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The regularization terms added to the loss function (J) encourages the weight matrix to diminish quantitavely\n",
    "- As the regularization hyperparameter **lambda** increases, the magnitude of the weight matrix **W** decreases\n",
    "    - This means more nodes are discarded (or made of less significance in the network) which kind of compells the model to generalize the training dataset\n",
    "- Also, making **lambda** very large can make the model to learn linear relationships only. This can make the model useless as it cannot learn complex (quadratic) relationships in the dataset\n",
    "    - e.g. For **tanh** activation, if the value of lambda is **very large**, it transforms the function to linearity inciting only linear relationships in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Dropout Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most commonly used for Convolutional Neural Networks (CNNs), **dropout regularization** is a technique to randomly remove nodes for each training data and iteration\n",
    "- To select if a node should be removed or not is chosen by using a probability randomness given by **keep_prob** hyperparameter\n",
    "    - e.g. If **keep_prob = 0.6**, then the chance for removing the node from that layer is 40%\n",
    "- The probability for node removal may vary for each layer\n",
    "\n",
    "```d = np.random.randn(activation.shape[0], activation.shape[1])\n",
    "activation = np.multiply(activation, d)\n",
    "activation /= keep_prob   # inverted dropout```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asheesh",
   "language": "python",
   "name": "asheesh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
